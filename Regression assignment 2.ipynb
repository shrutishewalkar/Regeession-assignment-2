{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bb33e3-a76f-4930-a690-4f36aefedbd7",
   "metadata": {},
   "source": [
    "Q1 Explain the concept of R-squared in linear regression models.how it is calculated , and what does it represented?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf3755-386c-4041-90bc-e38d44d38b71",
   "metadata": {},
   "source": [
    "Ans: R squared is a statistical measure that represents the proportion of variation in the dependent variable that is explained by the independent variables in a linear regression  model. it is used to evalute the goodness of fit of the model and to assess its predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbb13c-77d1-4f63-92ad-225f17643e42",
   "metadata": {},
   "source": [
    "R-squared ranges from 0 to 1 , with 1 including a perfect fit between the model and the data, and 0 indicating no relationship between the model and the data. A higher Rsquared value indicates that a large proportion of the variance in the dependent variable can be explained by thr independent variables in the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02bab8-4c41-4f7c-b7ab-c74194563910",
   "metadata": {},
   "source": [
    "Rsquared calculated as follows:\n",
    "    R^ = 1-RSS/TSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0e6db-c0f0-4c6f-9393-b54652e57b5e",
   "metadata": {},
   "source": [
    "Q2 Define adjusted Rsquared and explain how it differs from the regular R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46dbf36-9c1c-499a-9b30-11943576398e",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modification of the regular r squared that takes into account the number of indepent variables in a regression model. it is used to address the problem of overfitting , which occured when a model is too complex and includes variables that are not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef2ae6-055a-40ac-9534-886770a12a58",
   "metadata": {},
   "source": [
    "the adjusted Rsquared value is prefeered over the regular rsquared when comparing model with differerent number of independent variables. it provides a more accurate estimate of the explanatory power of the model by taking into account the tradeoff between the number of variables and the goodness of fit . A higher adjusted Rsquaresvalue indicates that the model explains a large proopoertions of the variance in the indepent variable while controlling for the number of independent variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca8814-d368-4371-806a-35088c2de50d",
   "metadata": {},
   "source": [
    "Q3 when it is more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271193e3-0f33-4f5e-a7fd-b086c9514392",
   "metadata": {},
   "source": [
    "The regular R-squared tends to increase as more independent variables are added to the model, even if those variables do not significantly improve the model's ability to explain the variation in the dependent variable. This can result in overfitting, where the model fits the sample data too closely and does not generalize well to new data.\n",
    "\n",
    "\n",
    "The adjusted R-squared, on the other hand, adjusts for the number of independent variables in the model and penalizes models that include irrelevant or unnecessary variables. Therefore, the adjusted R-squared is a more appropriate measure of model fit when comparing models with different numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1554b0-e030-42c3-ad6e-a92a7ca3727f",
   "metadata": {},
   "source": [
    "Q4 What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a207a-a000-463f-9dad-14e7e8488b43",
   "metadata": {},
   "source": [
    "Ans:RMSE (Root Mean Squared Error) is a measure of the average deviation of the predicted values from the actual values of the dependent variable. It is calculated as the square root of the average of the squared differences between the predicted and actual values of the dependent variable. RMSE is often preferred over MSE because it is in the same unit as the dependent variable, which makes it easier to interpret. The formula for RMSE is:\n",
    "\n",
    "\n",
    "RMSE = sqrt(mean((y_true - y_pred)^2))\n",
    "\n",
    "\n",
    "MSE (Mean Squared Error) is a measure of the average squared difference between the predicted and actual values of the dependent variable. It is calculated as the average of the squared differences between the predicted and actual values of the dependent variable. The formula for MSE is:\n",
    "\n",
    "\n",
    "MSE = mean((y_true - y_pred)^2)\n",
    "\n",
    "\n",
    "MAE (Mean Absolute Error) is a measure of the average absolute difference between the predicted and actual values of the dependent variable. It is calculated as the average of the absolute differences between the predicted and actual values of the dependent variable. MAE is less sensitive to outliers than MSE and RMSE. The formula for MAE is:\n",
    "\n",
    "\n",
    "MAE = mean(abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "All three metrics give an indication of how well the model fits the data, with lower values indicating better performance. The choice of which metric to use depends on the specific problem and the importance of the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a65646-c223-4e6e-a177-2f0868f11fb8",
   "metadata": {},
   "source": [
    "Q5 Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e4046-293f-48e9-b2c8-91f6ba2904a8",
   "metadata": {},
   "source": [
    "Ans:Advantages of RMSE:\n",
    "\n",
    "\n",
    "RMSE penalizes larger errors more than smaller errors, which can be desirable in some applications where larger errors have a greater impact.\n",
    "\n",
    "RMSE is in the same units as the dependent variable, which makes it easier to interpret and compare across different datasets or models.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "\n",
    "RMSE is more sensitive to outliers than MSE or MAE, which means that it may not be the best metric to use in situations where outliers are common or have a significant impact on the analysis.\n",
    "The square root in the formula for RMSE can make it difficult to work with mathematically.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "MSE is widely used because it has desirable mathematical properties, such as being convex and having a unique minimum.\n",
    "MSE is less sensitive to outliers than RMSE, which makes it a more robust metric in situations where outliers are common or have a significant impact on the analysis.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "MSE does not provide a direct measure of the size of the errors in the same units as the dependent variable, which makes it less intuitive to interpret than RMSE or MAE.\n",
    "MSE can be affected by the scale of the dependent variable, which can make it difficult to compare the performance of models across different datasets or models with different units of measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b9c34-d5d3-496b-bab8-f27fec8cc231",
   "metadata": {},
   "source": [
    "Advantages of MAE:\n",
    "\n",
    "\n",
    "MAE is less sensitive to outliers than RMSE and provides a direct measure of the size of the errors in the same units as the dependent variable, which makes it more intuitive to interpret.\n",
    "MAE can be used with non-linear regression models where the relationship between the dependent variable and the independent variables is not necessarily linear.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "MAE does not penalize larger errors more than smaller errors, which may not be desirable in some applications where larger errors have a greater impact.\n",
    "The absolute value in the formula for MAE can make it difficult to work with mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c166f6e-f31e-4ca2-b378-a4be059a3d1c",
   "metadata": {},
   "source": [
    "Q6 Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6ddd8-3758-4eca-9fc4-1d9937a70d69",
   "metadata": {},
   "source": [
    "Ans:Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique used in linear regression analysis to prevent overfitting and improve the predictive accuracy of the model.\n",
    "\n",
    "\n",
    "In Lasso regularization, the sum of the absolute values of the coefficients of the independent variables is added to the objective function being minimized. This penalty term shrinks the coefficients towards zero and can result in some coefficients being exactly equal to zero. As a result, Lasso can perform feature selection by removing less important variables from the model.\n",
    "\n",
    "\n",
    "Ridge regularization adds the sum of the squared values of the coefficients of the independent variables to the objective function. This penalty term shrinks the coefficients towards zero but does not set any coefficients exactly equal to zero. Thus, Ridge regularization does not perform feature selection.\n",
    "\n",
    "\n",
    "When to use Lasso or Ridge regularization depends on the nature of the problem and the available data. Lasso regularization is more appropriate when the data contains many independent variables, some of which may be less important or redundant. In such cases, Lasso regularization can help to identify and remove the less important variables from the model, resulting in a simpler and more interpretable model. Ridge regularization is more appropriate when all the independent variables are expected to have some impact on the dependent variable, and the goal is to reduce the variance of the estimates and improve the stability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7dfcd-7cea-4561-b59c-50b155050447",
   "metadata": {},
   "source": [
    "Q7 How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb172d9-7e79-4deb-a880-99ffde49ccd2",
   "metadata": {},
   "source": [
    "Ans: Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the cost function that discourages the model from overemphasizing the importance of any one feature, and encourages it to generalize to new, unseen data.\n",
    "\n",
    "\n",
    "For example, let's say we have a dataset with 10 features, and we want to predict the price of a house based on those features. We could use a linear regression model to make these predictions, but if we include all 10 features, the model may overfit to the training data, and not generalize well to new, unseen data.\n",
    "\n",
    "\n",
    "To prevent overfitting, we could use a regularized linear model, such as Ridge regression or Lasso regression. These models add a penalty term to the cost function, which penalizes large coefficients and encourages the model to select only the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fadd486-1d18-492e-a6d8-1587db0814f4",
   "metadata": {},
   "source": [
    "Q8 Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa1f55-b514-4d77-ac7a-84d780e05dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
